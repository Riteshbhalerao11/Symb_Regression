{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d4801-fadf-4269-9153-c23914b75373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "from utils import Model, generate_eqn_mask\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15099516-ae92-404b-b79e-099f00f44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token indices\n",
    "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX, SEP_IDX = 0, 1, 2, 3, 4\n",
    "\n",
    "# Special symbols list\n",
    "special_symbols = ['<s>', '<pad>', '</s>', '<unk>', '<sep>']\n",
    "\n",
    "# Device\n",
    "device = 'cuda'\n",
    "\n",
    "# Seed\n",
    "seed = 42\n",
    "\n",
    "# Checkpoint path for pretrained model\n",
    "ckp_path = \"Final_transformers.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341add9c-44f5-4bf3-941d-b5debe8e4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading features data\n",
    "data_directory = 'Data/Feynman_with_units'\n",
    "N = 60 # number of feature rows per equation \n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if os.path.isfile(os.path.join(data_directory, filename)):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for line in lines[:N]:\n",
    "                data.append((filename, line))\n",
    "                \n",
    "df = pd.DataFrame(data, columns=['Filename', 'features'])\n",
    "del data\n",
    "\n",
    "\n",
    "#loading target/equation data\n",
    "eq_df = pd.read_csv(\"Data/FeynmanEquations.csv\")[['Filename','Formula']]\n",
    "\n",
    "#merging features & target dataframes\n",
    "df = pd.merge(eq_df,df,on=\"Filename\",how='inner').drop(columns=['Filename'])\n",
    "del eq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc544b72-7283-4143-9dc8-14067e84235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get formulas from DataFrame\n",
    "fyn = df.Formula.tolist()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(fyn, special_symbols)\n",
    "\n",
    "# Build target vocabulary\n",
    "v = tokenizer.build_tgt_vocab()\n",
    "\n",
    "# Create dictionary mapping indices to tokens\n",
    "itos = {value: key for key, value in v.get_stoi().items()}\n",
    "\n",
    "# Calculate source and target vocabulary sizes\n",
    "src_voc_size = len(tokenizer.build_src_vocab())\n",
    "tgt_voc_size = len(v)\n",
    "\n",
    "del eq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c921f7b-6557-4e95-9e85-db3a066af13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations of the pretrained model\n",
    "\n",
    "model_config = {\n",
    "    \"emb_size\": 512,\n",
    "    \"dim_feedforward\": 3072,\n",
    "    \"nhead\": 8,\n",
    "    \"num_encoder_layers\": 4,\n",
    "    \"num_decoder_layers\": 4,\n",
    "    \"tgt_vocab_size\" : tgt_voc_size,\n",
    "    'src_vocab_size' : src_voc_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622370c5-eec8-40de-bb8e-62f0195ec190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    \"\"\"\n",
    "    Class for generating predictions using a trained model.\n",
    "\n",
    "    Args:\n",
    "        device (str): Device to use for inference.\n",
    "        epoch (int): Epoch number.\n",
    "\n",
    "    Attributes:\n",
    "        model (Model): Trained model for prediction.\n",
    "        path (str): Path to the trained model.\n",
    "        device (str): Device for inference.\n",
    "        df (DataFrame): DataFrame containing training data.\n",
    "        vocab (dict): Vocabulary for tokenization.\n",
    "        attrs (list): List of attributes in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.vocab = {}\n",
    "        self.attrs = ['features', 'Formula']\n",
    "        \n",
    "        self.vocab[self.attrs[0]] = tokenizer.build_src_vocab()\n",
    "        self.vocab[self.attrs[1]] = tokenizer.build_tgt_vocab()\n",
    "\n",
    "    def tok_to_id(self, tokenize, vocab, val):\n",
    "        \"\"\"\n",
    "        Convert tokens to token IDs using the provided tokenizer and vocabulary.\n",
    "\n",
    "        Args:\n",
    "            tokenize (function): Tokenization function.\n",
    "            vocab (function): Vocabulary function.\n",
    "            val (str): Input string.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Token IDs.\n",
    "        \"\"\"\n",
    "        val = tokenize(val)\n",
    "        token_ids = vocab(val)\n",
    "        return torch.tensor(token_ids, dtype=torch.int)\n",
    "\n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol):\n",
    "        \"\"\"\n",
    "        Generate a sequence using greedy decoding.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source input.\n",
    "            src_mask (Tensor): Mask for source input.\n",
    "            max_len (int): Maximum length of the generated sequence.\n",
    "            start_symbol (int): Start symbol for decoding.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence.\n",
    "        \"\"\"\n",
    "        src = src.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        dim = 1\n",
    "\n",
    "        memory = self.model.encode(src, src_mask)\n",
    "        memory = memory.to(self.device)\n",
    "        dim = 0\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n",
    "        for i in range(max_len - 1):\n",
    "            tgt_mask = (generate_eqn_mask(ys.size(0), self.device).type(torch.bool)).to(self.device)\n",
    "            out = self.model.decode(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            prob = self.model.generator(out[:, -1])\n",
    "\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=dim)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "        return ys\n",
    "\n",
    "    def predict(self, test_example, raw_tokens=False):\n",
    "        \"\"\"\n",
    "        Generate prediction for a test example.\n",
    "\n",
    "        Args:\n",
    "            test_example (dict): Test example containing input features.\n",
    "            raw_tokens (bool, optional): Whether to return raw tokens. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str or tuple: Decoded equation or tuple of original and predicted tokens.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        src_sentence = test_example[self.attrs[0]]\n",
    "\n",
    "        src = self.tok_to_id(tokenizer.src_tokenize, self.vocab[self.attrs[0]], src_sentence).view(-1, 1)\n",
    "        num_tokens = src.shape[0]\n",
    "\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = self.greedy_decode(src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "\n",
    "        if raw_tokens:\n",
    "            original_sentence = test_example[self.attrs[1]]\n",
    "            original_tokens = self.tok_to_id(tokenizer.tgt_tokenize, self.vocab[self.attrs[1]], original_sentence)\n",
    "            return original_tokens, tgt_tokens\n",
    "\n",
    "        decoded_eqn = ''\n",
    "        for t in tgt_tokens:\n",
    "            decoded_eqn += itos[int(t)]\n",
    "\n",
    "        return decoded_eqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8e0b7-dda5-4325-bdba-17d49ab6b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_accuracy(model, frac=0.1):\n",
    "    \"\"\"\n",
    "    Compute the sequence accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to evaluate.\n",
    "        frac (float): Fraction of data to sample for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: Sequence accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Initialize Predictor with the model\n",
    "    predictor = Predictor(model)\n",
    "    \n",
    "    # Initialize count variable for accurate predictions\n",
    "    count = 0\n",
    "    \n",
    "    random_df = df.sample(frac=frac, random_state=seed)\n",
    "    length = len(random_df)\n",
    "    \n",
    "    pbar = tqdm(range(length))\n",
    "    pbar.set_description(\"Seq_Acc_Cal\")\n",
    "    \n",
    "    for i in pbar:\n",
    "        original_tokens, predicted_tokens = predictor.predict(random_df.iloc[i], raw_tokens=True)\n",
    "\n",
    "        original_tokens = original_tokens.tolist()\n",
    "        predicted_tokens = predicted_tokens.tolist()\n",
    "\n",
    "        if original_tokens == predicted_tokens:\n",
    "            count = count + 1\n",
    "\n",
    "        pbar.set_postfix(seq_accuracy=count / (i + 1))\n",
    "\n",
    "    return count / length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034786bc-42cc-4233-898d-6b36a0238d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    \"\"\"\n",
    "    Function to instantiate a Model object and initialize its parameters using \n",
    "    previously defined global variables.\n",
    "\n",
    "    Returns:\n",
    "        Model: Initialized model object.\n",
    "    \"\"\"\n",
    "    model = Model(**config)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57349606-be7d-463e-b794-4c8f95cf9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4812da-41b5-497f-81af-b23fc1fcbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(ckp_path)\n",
    "model.load_state_dict(state['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d09e82-3a2e-4767-bb26-4de26d58276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_accuracy(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ritesh-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

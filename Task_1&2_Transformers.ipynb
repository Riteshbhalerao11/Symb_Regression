{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a00df-5ad4-468b-b439-dbd58a48a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import pickle\n",
    "import dill\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd61a7-48c1-4413-8456-1f6de8277fd6",
   "metadata": {},
   "source": [
    "# Common Task 1. Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668e2f1-71e2-435d-be86-ec39b6f2fcc8",
   "metadata": {},
   "source": [
    "### Declaring Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89377ccd-bd6c-4ee8-b2a4-c74cc8d88859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special tokens & coressponding ids\n",
    "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX , SEP_IDX = 0, 1, 2, 3, 4\n",
    "special_symbols = ['<s>', '<pad>', '</s>', '<unk>', '<sep>'] \n",
    "\n",
    "# Directory where data and model checkpoints will be stored\n",
    "root_dir = \"./\"\n",
    "\n",
    "# Device for training (e.g., \"cuda\" for GPU, \"cpu\" for CPU)\n",
    "device = \"cuda\"\n",
    "\n",
    "# Epochs at which to save model checkpoints during training\n",
    "save_at_epochs = []\n",
    "\n",
    "# Total number of epochs for training\n",
    "epochs = 40\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Whether to use half precision (FP16) for training\n",
    "use_half_precision = True\n",
    "\n",
    "# Whether to shuffle training data during each epoch\n",
    "train_shuffle = True\n",
    "\n",
    "# Whether to shuffle test data\n",
    "test_shuffle = False\n",
    "\n",
    "# Batch size for training\n",
    "training_batch_size = 64\n",
    "\n",
    "# Batch size for testing\n",
    "test_batch_size = 128\n",
    "\n",
    "# Number of worker processes for data loading\n",
    "num_workers = 4\n",
    "\n",
    "# Tokenizer for text processing (e.g., \"bert-base-uncased\", \"gpt2\", etc.)\n",
    "tokenizer = None\n",
    "\n",
    "# Paths to training, testing, and validation datasets\n",
    "train = None\n",
    "test = None\n",
    "valid = None\n",
    "\n",
    "# Size of vocabulary for source and target sequences\n",
    "src_voc_size = None\n",
    "tgt_voc_size = None\n",
    "\n",
    "# Whether to use pinned memory for data loading (faster on GPU)\n",
    "pin_memory = True\n",
    "\n",
    "# Learning rate for optimizer\n",
    "optimizer_lr = 0.0001\n",
    "\n",
    "# Gradient clipping threshold (set to -1 to disable)\n",
    "clip_grad_norm = -1\n",
    "\n",
    "# Name of the sequence-to-sequence model architecture\n",
    "model_name = \"seqtoseq_basic\"\n",
    "\n",
    "# Dimensionality of word embeddings\n",
    "embedding_size = 512\n",
    "\n",
    "# Dimensionality of hidden layers in the transformer model\n",
    "hidden_dim = 3072\n",
    "\n",
    "# Number of attention heads in the transformer model\n",
    "nhead = 8\n",
    "\n",
    "# Number of encoder layers in the transformer model\n",
    "num_encoder_layers = 4\n",
    "\n",
    "# Number of decoder layers in the transformer model\n",
    "num_decoder_layers = 4\n",
    "\n",
    "# Dropout probability for regularization\n",
    "dropout = 0.1\n",
    "\n",
    "# Current epoch number (used for resuming training)\n",
    "curr_epoch = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69a29c-baf0-4a5d-bf28-4c44e7b72f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading features data\n",
    "data_directory = 'Data/Feynman_with_units'\n",
    "N = 10 # number of feature rows per equation \n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(data_directory):\n",
    "    if os.path.isfile(os.path.join(data_directory, filename)):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for line in lines[:N]:\n",
    "                data.append((filename, line))\n",
    "                \n",
    "df = pd.DataFrame(data, columns=['Filename', 'features'])\n",
    "del data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8bc16a-2315-44dd-bf47-47999e0c7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading target/equation data\n",
    "eq_df = pd.read_csv(\"Data/FeynmanEquations.csv\")[['Filename','Formula']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c75434-a1a7-4139-b7e2-3e9aff437298",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1889d-d7f3-4747-9a50-215b0ac42546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging features & target dataframes\n",
    "df = pd.merge(eq_df,df,on=\"Filename\",how='inner').drop(columns=['Filename'])\n",
    "del eq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab789c-9441-4134-a8d9-baa0118d29ad",
   "metadata": {},
   "source": [
    "## Tokenization explanation\n",
    "\n",
    "**Input Sequence Tokenization:**\n",
    "For the input sequences, tokenization is straightforwardâ€”character-level tokenization is employed, where each character is treated as a token. This approach, combined with positional embeddings, provides the model with a robust representation of the diverse combinations present within these sequences. Experimentally, I found out that character-level tokenization outperform by slight margins, subword tokenization based on n-gram frequency.\n",
    "\n",
    "\n",
    "For example : The input sequence `1.70 -0.093` will be tokenized as <mark>['&lt;s>', '1', '.', '7', '0', '&lt;sep>', '-', '0', '.', '0', '9', '3', '&lt;/s>']\n",
    "</mark>\n",
    "\n",
    "\n",
    "**Output/Target Sequence Tokenization:**\n",
    "For the output/target sequences, several steps are followed to obtain the final tokens. Firstly, the tokens are preprocessed to ensure a consistent representation of the equations. Subsequently, tokenization is performed in such a way that each variable, operator, and parenthesis is assigned a separate token. This results in final tokens that carry both physical and semantic meaning. Moreover, these tokens have been observed to perform slightly better than subword tokens based on n-gram frequency or character-level tokens.\n",
    "\n",
    "\n",
    "For example : The output sequence `exp(-theta**2/2)` will be tokenized to <mark>[&lt;s>, exp, ' ', ( , ' ', -, ' ', theta, ^, 2, ' ', /, ' ', 2 , ), &lt;/s>']</mark>\n",
    "\n",
    "<br>\n",
    "\n",
    "### Class explanation\n",
    "\n",
    "1. **Initialization**: The `Tokenizer` class is initialized with a list of equations (`eqns`) to tokenize. Regular expressions are defined to identify different components of the equations, including identifiers, numbers, operators, and spaces.\n",
    "\n",
    "2. **Token Extraction (_extract)**: The `_extract` method is responsible for extracting tokens from an equation and updating a dictionary with token counts. It uses regular expressions to find all occurrences of tokens within the equation.\n",
    "\n",
    "3. **Preprocessing (_preprocess_eqn)**: The `_preprocess_eqn` method preprocesses the equation by replacing **'*\\*'** with **'^'** and adding spaces around operators. This step ensures uniformity in tokenization and helps in separating different components of the equation.\n",
    "\n",
    "4. **Building Vocabulary (build_tgt_vocab and build_src_vocab)**: The `build_tgt_vocab` and `build_src_vocab` methods build vocabularies for target and source sequences, respectively. They iterate through each equation, preprocess it, extract tokens using regular expressions, and update an ordered dictionary with token counts. Finally, a vocabulary is created using the `torchtext.vocab.vocab` class.\n",
    "\n",
    "5. **Source Tokenization (src_tokenize)**: The `src_tokenize` method tokenizes source equations. It replaces digits with semicolons around them, adds separators around operators, and splits the equation into tokens. Special tokens '&lt;s>' and '&lt;/s>' are added at the beginning and end of the token list, respectively.\n",
    "\n",
    "6. **Target Tokenization (tgt_tokenize)**: The `tgt_tokenize` method tokenizes target equations. It adds separators around identifiers and operators, splits the equation into tokens, & adds special tokens '&lt;s>' and '&lt;/s>' at the beginning and end of the token list, respectively.\n",
    "\n",
    "7. **Source Decoding (src_decode)**: The `src_decode` method decodes tokens of a source equation into a string representation. It removes special tokens '&lt;s>' and '&lt;/s>', replaces '&lt;sep>' with a space, and removes any remaining semicolons. Additionally, it removes spaces after minus signs.\n",
    "\n",
    "8. **Target Decoding (tgt_decode)**: The `tgt_decode` method decodes tokens of a target equation into a string representation. It removes special tokens '&lt;s>' and '&lt;/s>' and replaces '&lt;sep>' with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab5df6-e956-4bb1-a9ae-ba797ddd6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Class for tokenizing equations and building vocabularies for source and target sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eqns):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer with equations.\n",
    "        Args:\n",
    "            eqns (list): List of equations to tokenize.\n",
    "        \"\"\"\n",
    "        self.eqns = eqns\n",
    "        self.identifier = r'[a-zA-Z_][a-zA-Z_0-9]*'\n",
    "        self.number = r'[0-9]+(?:\\.[0-9]*)?'\n",
    "        self.operator = r'\\^|[-+*/=<>()]'\n",
    "        self.space = r'[ \\t]+'\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract(eqn, ord_dict, pattern):\n",
    "        \"\"\"\n",
    "        Extracts tokens from the equation and updates the dictionary with token counts.\n",
    "        \"\"\"\n",
    "        elems = pattern.findall(eqn)\n",
    "        elem_counts = Counter(elems)\n",
    "        for key, value in elem_counts.items():\n",
    "            if key not in ord_dict:\n",
    "                ord_dict[key] = value\n",
    "            else:\n",
    "                ord_dict[key] += value\n",
    "        return ord_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def add_separators(match):\n",
    "        \"\"\"\n",
    "        Adds separators around operators in the equation for easier splitting.\n",
    "        \"\"\"\n",
    "        return f\";{match.group(0)};\"\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_eqn(data):\n",
    "        \"\"\"\n",
    "        Preprocesses the equation by replacing '**' with '^' and adding spaces around operators.\n",
    "        \"\"\"\n",
    "        data = re.sub(r'\\*\\*', '^', data)\n",
    "        for r in (('{', ' {'), ('}', '}'), (' + ', ' + '), ('*', ' * '), ('-', ' - '), ('+', ' + '),\n",
    "                  ('^', '^'), ('(', ' ('), (')', ')'), ('/', ' / '), ('  ', ' '), (' - ', ' - '), ('( (', '((')):\n",
    "            data = data.replace(*r)\n",
    "        return data.strip()\n",
    "\n",
    "    def build_tgt_vocab(self):\n",
    "        \"\"\"\n",
    "        Builds vocabulary for target sequences.\n",
    "        \"\"\"\n",
    "        ordered_dict = OrderedDict()\n",
    "        exps = [self.space, self.identifier, self.number, self.operator]\n",
    "        for eqn in self.eqns:\n",
    "            eqn = self.preprocess_eqn(eqn)\n",
    "            for exp in exps:\n",
    "                ordered_dict = self._extract(eqn, ordered_dict, re.compile(exp))\n",
    "        voc = vocab(ordered_dict, specials=special_symbols[:-1], special_first=True)\n",
    "        voc.set_default_index(UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def build_src_vocab(self):\n",
    "        \"\"\"\n",
    "        Builds vocabulary for source sequences.\n",
    "        \"\"\"\n",
    "        ordered_dict = OrderedDict()\n",
    "        for i in range(10):\n",
    "            ordered_dict[str(i)] = 1\n",
    "        ordered_dict['-'] = 1\n",
    "        ordered_dict['.'] = 1\n",
    "        voc = vocab(ordered_dict, specials=special_symbols, special_first=True)\n",
    "        voc.set_default_index(UNK_IDX)\n",
    "        return voc\n",
    "\n",
    "    def src_tokenize(self, expr):\n",
    "        \"\"\"\n",
    "        Tokenizes source equations.\n",
    "        \"\"\"\n",
    "        pattern = r'\\d'\n",
    "        expr = re.sub(pattern, r';\\g<0>;', expr)\n",
    "        expr = re.sub(r';{2,}', ';', expr)  # Replace multiple semicolons with single semicolon\n",
    "        expr = re.sub(r'\\s+', '<sep>', expr)\n",
    "        expr = re.sub('-', ';-;', expr)\n",
    "        expr = re.sub(r';{2,}', ';', expr)\n",
    "        toks = expr.split(';')\n",
    "        toks[0] = '<s>'\n",
    "        toks[-1] = '</s>'\n",
    "        return toks\n",
    "\n",
    "    def tgt_tokenize(self, expr):\n",
    "        \"\"\"\n",
    "        Tokenizes target equations.\n",
    "        \"\"\"\n",
    "        exps = [self.space, self.identifier, self.operator]\n",
    "        expr = self.preprocess_eqn(expr)\n",
    "        for exp in exps:\n",
    "            expr = re.sub(exp, self.add_separators, expr)\n",
    "        expr = re.sub(r';{2,}', ';', expr)  # Replace multiple semicolons with single semicolon\n",
    "        toks = expr.split(';')\n",
    "        toks[0] = '<s>'\n",
    "        toks[-1] = '</s>'\n",
    "        return toks\n",
    "\n",
    "    def src_decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Decodes source tokens into equation string.\n",
    "        \"\"\"\n",
    "        expr = ' '.join(tokens[1:-1])  # Exclude <s> and </s> tokens\n",
    "        expr = expr.replace('<sep>', ' ')\n",
    "        expr = expr.replace(';', '')  # Remove any remaining semicolons\n",
    "        expr = expr.replace('- ', '-')  # Remove space after minus sign\n",
    "        return expr\n",
    "\n",
    "    def tgt_decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Decodes target tokens into equation string.\n",
    "        \"\"\"\n",
    "        expr = ''.join(tokens[1:-1])  # Exclude <s> and </s> tokens\n",
    "        expr = expr.replace('<sep>', ' ')\n",
    "        return expr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c6b0a-589a-4708-b246-ee28985329bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fyn_eqs = df.Formula.tolist()\n",
    "\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "df_valid = pd.DataFrame()\n",
    "\n",
    "for i in range(len(set(fyn_eqs))):\n",
    "    dat = df.iloc[i * N: N * (i + 1)].sample(frac=1, random_state=seed)\n",
    "    total_len = len(dat)\n",
    "    train_len = int(0.9 * total_len)\n",
    "    test_len = int(0.05 * total_len)  # Remaining 5% for test and valid splits\n",
    "    valid_len = total_len - train_len - test_len\n",
    "    \n",
    "    df_train = pd.concat([df_train, dat.iloc[:train_len]])\n",
    "    df_test = pd.concat([df_test, dat.iloc[train_len:train_len + test_len]])\n",
    "    df_valid = pd.concat([df_valid, dat.iloc[train_len + test_len:]])\n",
    "\n",
    "# Delete unnecessary variable\n",
    "del dat\n",
    "\n",
    "# Assign data to splits\n",
    "df_train.reset_index(drop=True,inplace=True)\n",
    "df_test.reset_index(drop=True,inplace=True)\n",
    "df_valid.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "# Defining target & source vocabulary sizes & id-to-string mapping for target sequence\n",
    "tokenizer = Tokenizer(fyn_eqs)\n",
    "v = tokenizer.build_tgt_vocab()\n",
    "itos = {value: key for key, value in v.get_stoi().items()}\n",
    "src_voc_size = len(tokenizer.build_src_vocab())\n",
    "tgt_voc_size = len(tokenizer.build_tgt_vocab())\n",
    "\n",
    "# Delete the original DataFrame\n",
    "del df, fyn_eqs, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40827c8c-5579-48fc-b599-31cb8eff3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment & execute to save tokenzier\n",
    "\n",
    "# #Saving our tokenizer\n",
    "# with open('tokenizer.pkl', 'wb') as f:\n",
    "#     dill.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc697a-f845-40ee-9610-318bbe467110",
   "metadata": {},
   "source": [
    "# Common Task 2: Train/Evaluate Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c6e0e-34ce-457f-8b88-731482f35209",
   "metadata": {},
   "source": [
    "## Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952e800-b046-402e-bb0e-7b67b0c8bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eqn_mask(n: int, device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate an equation mask for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        n (int): The size of the mask.\n",
    "        device (torch.device): The device on which to create the mask.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The equation mask.\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones((n, n), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6aeaf-ffc6-465f-942f-2afeab685f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src: torch.Tensor, tgt: torch.Tensor, device: torch.device) -> tuple:\n",
    "    \"\"\"\n",
    "    Create masks for source and target sequences.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Source sequence.\n",
    "        tgt (torch.Tensor): Target sequence.\n",
    "        device (torch.device): Device on which to create the masks.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing four masks: source mask, target mask, source padding mask, target padding mask.\n",
    "    \"\"\"\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # Generate equation mask for target sequence\n",
    "    tgt_mask = generate_eqn_mask(tgt_seq_len, device)\n",
    "    \n",
    "    # Create source mask\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    # Create source and target padding masks\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7b814-e427-4ae1-b831-120269f9bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Collate function for batching sequences.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples containing source and target sequences.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing padded source batch and padded target batch.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for (src_sample, tgt_sample) in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    # Pad sequences in the batch\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    \n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7e46d-b2ac-45e0-b2b0-5c2fdc6a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_accuracy(load_best=True, epochs=None):\n",
    "    \"\"\"\n",
    "    Calculate the sequence accuracy.\n",
    "\n",
    "    Args:\n",
    "        load_best (bool, optional): Whether to load the best model. Defaults to True.\n",
    "        epochs (int, optional): Number of epochs. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: Sequence accuracy.\n",
    "    \"\"\"\n",
    "    predictor = Predictor(load_best, epochs)\n",
    "    count = 0\n",
    "    random_df = df_test.sample(frac=0.1, random_state=seed)\n",
    "    length = len(random_df)\n",
    "    pbar = tqdm(range(length))\n",
    "    pbar.set_description(\"Seq_Acc_Cal\")\n",
    "    for i in pbar:\n",
    "        original_tokens, predicted_tokens = predictor.predict(random_df.iloc[i], raw_tokens=True)\n",
    "        original_tokens = original_tokens.tolist()\n",
    "        predicted_tokens = predicted_tokens.tolist()\n",
    "        if original_tokens == predicted_tokens:\n",
    "            count = count + 1\n",
    "        pbar.set_postfix(seq_accuracy=count / (i + 1))\n",
    "    return count / length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce39107-8bbd-4b22-9938-b6aabce49cb8",
   "metadata": {},
   "source": [
    "## Defining required classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c908c-2557-47c7-b60f-50226e5cbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper modules for network architecture\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module for transformer architectures.\n",
    "\n",
    "    Args:\n",
    "        emb_size (int): The embedding size.\n",
    "        dropout (float): Dropout rate.\n",
    "        maxlen (int, optional): Maximum sequence length. Defaults to 5000.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Token embedding module for transformer architectures.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of the vocabulary.\n",
    "        emb_size (int): The embedding size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba08ae8-7621-4356-a773-a282377f6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch dataset for handling data.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.tgt_vals = df['Formula']\n",
    "        self.src_vals = df['features']\n",
    "        self.tgt_vocab = tokenizer.build_tgt_vocab()\n",
    "        self.src_vocab = tokenizer.build_src_vocab()\n",
    "        self.tgt_tokenize = tokenizer.tgt_tokenize\n",
    "        self.src_tokenize = tokenizer.src_tokenize\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.src_vals)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing source and target tensors.\n",
    "        \"\"\"\n",
    "        src_tokenized = self.src_tokenize(self.src_vals[idx])\n",
    "        tgt_tokenized = self.tgt_tokenize(self.tgt_vals[idx])\n",
    "        src_ids = self.src_vocab(src_tokenized)\n",
    "        tgt_ids = self.tgt_vocab(tgt_tokenized)\n",
    "\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        \"\"\"\n",
    "        Create datasets (train, test, and valid)\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing train, test, and valid datasets.\n",
    "        \"\"\"\n",
    "        train = Dataset(df_train)\n",
    "        test = Dataset(df_test)\n",
    "        valid = Dataset(df_valid)\n",
    "\n",
    "        return {'train': train, 'test': test, 'valid': valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e0db2-c9c6-4f27-b6ad-e07b5d1d68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based model for sequence-to-sequence tasks.\n",
    "\n",
    "    Args:\n",
    "        num_encoder_layers (int): Number of encoder layers.\n",
    "        num_decoder_layers (int): Number of decoder layers.\n",
    "        emb_size (int): Size of the embedding.\n",
    "        nhead (int): Number of attention heads.\n",
    "        src_vocab_size (int): Size of the source vocabulary.\n",
    "        tgt_vocab_size (int): Size of the target vocabulary.\n",
    "        dim_feedforward (int, optional): Dimension of the feedforward network. Defaults to 512.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Model, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source input.\n",
    "            trg (Tensor): Target input.\n",
    "            src_mask (Tensor): Mask for source input.\n",
    "            tgt_mask (Tensor): Mask for target input.\n",
    "            src_padding_mask (Tensor): Padding mask for source input.\n",
    "            tgt_padding_mask (Tensor): Padding mask for target input.\n",
    "            memory_key_padding_mask (Tensor): Padding mask for memory.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "            src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Encode the source input.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source input.\n",
    "            src_mask (Tensor): Mask for source input.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded tensor.\n",
    "        \"\"\"\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        \"\"\"\n",
    "        Decode the target input.\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): Target input.\n",
    "            memory (Tensor): Memory tensor.\n",
    "            tgt_mask (Tensor): Mask for target input.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Decoded tensor.\n",
    "        \"\"\"\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68451d20-b4d5-4510-9475-8f16d456c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"\n",
    "    Function to instantiate a Model object and initialize its parameters using \n",
    "    previously defined global variables.\n",
    "\n",
    "    Returns:\n",
    "        Model: Initialized model object.\n",
    "    \"\"\"\n",
    "    model = Model(num_encoder_layers, num_decoder_layers, embedding_size,\n",
    "                  nhead, src_voc_size, tgt_voc_size, hidden_dim, dropout)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065eebdb-bd7e-4835-bcb9-7bf6b5f2bf54",
   "metadata": {},
   "source": [
    "## Designing the Prediction & Training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e6e5b-c628-44e6-a35d-bee19aa20f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    \"\"\"\n",
    "    Class for generating predictions using a trained model.\n",
    "\n",
    "    Args:\n",
    "        device (str): Device to use for inference.\n",
    "        epoch (int): Epoch number.\n",
    "\n",
    "    Attributes:\n",
    "        model (Model): Trained model for prediction.\n",
    "        path (str): Path to the trained model.\n",
    "        device (str): Device for inference.\n",
    "        df (DataFrame): DataFrame containing training data.\n",
    "        vocab (dict): Vocabulary for tokenization.\n",
    "        attrs (list): List of attributes in the dataset.\n",
    "        checkpoint (str): model checkpoint path\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,load_best=True,epochs=None):\n",
    "        self.model = get_model()\n",
    "        self.checkpoint = f\"model_best_tr.pth\" if load_best else f\"model_ep{epochs+1}.pth\"\n",
    "        self.path = os.path.join(root_dir,self.checkpoint)\n",
    "        self.device = device\n",
    "        self.df = train\n",
    "        state = torch.load(self.path, map_location=self.device)\n",
    "        self.model.load_state_dict(state['state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.vocab = {}\n",
    "        self.attrs = ['features', 'Formula']\n",
    "        \n",
    "        self.vocab[self.attrs[0]] = tokenizer.build_src_vocab()\n",
    "        self.vocab[self.attrs[1]] = tokenizer.build_tgt_vocab()\n",
    "\n",
    "    def tok_to_id(self, tokenize, vocab, val):\n",
    "        \"\"\"\n",
    "        Convert tokens to token IDs using the provided tokenizer and vocabulary.\n",
    "\n",
    "        Args:\n",
    "            tokenize (function): Tokenization function.\n",
    "            vocab (function): Vocabulary function.\n",
    "            val (str): Input string.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Token IDs.\n",
    "        \"\"\"\n",
    "        val = tokenize(val)\n",
    "        token_ids = vocab(val)\n",
    "        return torch.tensor(token_ids, dtype=torch.int)\n",
    "\n",
    "    def greedy_decode(self, src, src_mask, max_len, start_symbol):\n",
    "        \"\"\"\n",
    "        Generate a sequence using greedy decoding.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source input.\n",
    "            src_mask (Tensor): Mask for source input.\n",
    "            max_len (int): Maximum length of the generated sequence.\n",
    "            start_symbol (int): Start symbol for decoding.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Generated sequence.\n",
    "        \"\"\"\n",
    "        src = src.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        dim = 1\n",
    "\n",
    "        memory = self.model.encode(src, src_mask)\n",
    "        memory = memory.to(self.device)\n",
    "        dim = 0\n",
    "        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(self.device)\n",
    "        for i in range(max_len - 1):\n",
    "            tgt_mask = (generate_eqn_mask(ys.size(0), self.device).type(torch.bool)).to(self.device)\n",
    "            out = self.model.decode(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            prob = self.model.generator(out[:, -1])\n",
    "\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "            ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=dim)\n",
    "            if next_word == EOS_IDX:\n",
    "                break\n",
    "        return ys\n",
    "\n",
    "    def predict(self, test_example, raw_tokens=False):\n",
    "        \"\"\"\n",
    "        Generate prediction for a test example.\n",
    "\n",
    "        Args:\n",
    "            test_example (dict): Test example containing input features.\n",
    "            raw_tokens (bool, optional): Whether to return raw tokens. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            str or tuple: Decoded equation or tuple of original and predicted tokens.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        src_sentence = test_example[self.attrs[0]]\n",
    "\n",
    "        src = self.tok_to_id(tokenizer.src_tokenize, self.vocab[self.attrs[0]], src_sentence).view(-1, 1)\n",
    "        num_tokens = src.shape[0]\n",
    "\n",
    "        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "        tgt_tokens = self.greedy_decode(src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "\n",
    "        if raw_tokens:\n",
    "            original_sentence = test_example[self.attrs[1]]\n",
    "            original_tokens = self.tok_to_id(tokenizer.tgt_tokenize, self.vocab[self.attrs[1]], original_sentence)\n",
    "            return original_tokens, tgt_tokens\n",
    "\n",
    "        decoded_eqn = ''\n",
    "        for t in tgt_tokens:\n",
    "            decoded_eqn += itos[int(t)]\n",
    "\n",
    "        return decoded_eqn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5bb77-5b80-40a6-80e3-1ee5250b9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\"\n",
    "    Class for training a sequence-to-sequence model.\n",
    "\n",
    "    Args:\n",
    "        start_epoch (int, optional): Starting epoch number. Defaults to 0.\n",
    "\n",
    "    Attributes:\n",
    "        scaler (GradScaler): Gradient scaler for half-precision training.\n",
    "        dtype (torch.dtype): Data type for training.\n",
    "        dataloaders (dict): Dataloaders for train, validation, and test datasets.\n",
    "        root_dir (str): Root directory for saving models and logs.\n",
    "        device (str): Device for training.\n",
    "        current_epoch (int): Current epoch number.\n",
    "        best_val_loss (float): Best validation loss.\n",
    "        train_loss_list (list): List of training losses.\n",
    "        valid_loss_list (list): List of validation losses.\n",
    "        valid_accuracy_tok_list (list): List of validation token accuracies.\n",
    "        model (Model): Model for training.\n",
    "        optimizer (Optimizer): Optimizer for training.\n",
    "        scheduler (Scheduler): Learning rate scheduler.\n",
    "        resume_best (bool): Whether to resume from the last best saved model\n",
    "        save_freq (int): Frequency of saving in terms of epochs\n",
    "        save_last (bool): Whether to save model after complete training\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resume_best=False, save_freq=None,save_last = True, start_epoch=0):\n",
    "\n",
    "        # For half precision training\n",
    "        self.scaler = GradScaler()\n",
    "        if use_half_precision:\n",
    "            self.dtype = torch.float16\n",
    "        else:\n",
    "            self.dtype = torch.float32\n",
    "\n",
    "        self.dataloaders = self._prepare_dataloaders()\n",
    "        self.root_dir = root_dir\n",
    "        self.device = device\n",
    "        self.current_epoch = start_epoch\n",
    "        self.best_val_loss = 1e6\n",
    "        self.train_loss_list = []\n",
    "        self.valid_loss_list = []\n",
    "        self.valid_accuracy_tok_list = []\n",
    "        self.model = self._prepare_model()\n",
    "        self.optimizer = self._prepare_optimizer()\n",
    "        self.scheduler = self._prepare_scheduler()\n",
    "        self.save_freq = save_freq\n",
    "        self.resume_best = resume_best\n",
    "        self.save_last = save_last\n",
    "\n",
    "    def criterion(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Calculate the loss between predicted and true values.\n",
    "\n",
    "        Args:\n",
    "            y_pred (Tensor): Predicted values.\n",
    "            y_true (Tensor): True values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Loss value.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        return loss_fn(y_pred, y_true)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make a forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        \"\"\"\n",
    "        Initialize and prepare the model for training.\n",
    "\n",
    "        Returns:\n",
    "            Model: Initialized model.\n",
    "        \"\"\"\n",
    "        model = get_model()\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    def _prepare_optimizer(self):\n",
    "        \"\"\"\n",
    "        Initialize the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            Optimizer: Initialized optimizer.\n",
    "        \"\"\"\n",
    "        param_optimizer = list(self.model.parameters())\n",
    "        optimizer = torch.optim.Adam(param_optimizer, lr=optimizer_lr, eps=1e-9)\n",
    "        return optimizer\n",
    "\n",
    "    def _prepare_scheduler(self):\n",
    "        \"\"\"\n",
    "        Initialize the learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            Scheduler: Initialized scheduler.\n",
    "        \"\"\"\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=2)\n",
    "        return scheduler\n",
    "\n",
    "    def _prepare_dataloaders(self):\n",
    "        \"\"\"\n",
    "        Prepare dataloaders for training, validation, and testing.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing train, validation, and test dataloaders.\n",
    "        \"\"\"\n",
    "        datasets = Dataset.get_data()\n",
    "        train_loader = torch.utils.data.DataLoader(datasets['train'], batch_size=training_batch_size,\n",
    "                                                   shuffle=train_shuffle, num_workers=num_workers,\n",
    "                                                   pin_memory=pin_memory, collate_fn=collate_fn)\n",
    "\n",
    "        dataloaders = {\n",
    "            'train': train_loader,\n",
    "            'valid': torch.utils.data.DataLoader(datasets['valid'],\n",
    "                                                  batch_size=test_batch_size, shuffle=test_shuffle,\n",
    "                                                  num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn),\n",
    "            'test': torch.utils.data.DataLoader(datasets['test'],\n",
    "                                                 batch_size=test_batch_size, shuffle=test_shuffle,\n",
    "                                                 num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn),\n",
    "        }\n",
    "        return dataloaders\n",
    "\n",
    "    def load_model(self, resume=False, epoch=None):\n",
    "        \"\"\"\n",
    "        Load the most recent model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            resume (bool, optional): Whether to resume training. Defaults to False.\n",
    "            epoch (int, optional): Load model from a particular epoch\n",
    "        \"\"\"\n",
    "        checkpoint_name = f\"model_best_tr.pth\" if epoch is None else f\"model_ep{epoch+1}.pth\"\n",
    "        file = os.path.join(self.root_dir, checkpoint_name)\n",
    "        state = torch.load(file, map_location=self.device)\n",
    "        self.model.load_state_dict(state['state_dict'])\n",
    "        if resume:\n",
    "            self.optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        \"\"\"\n",
    "        Perform a single training epoch.\n",
    "\n",
    "        Returns:\n",
    "            float: Average training loss for the epoch.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        pbar = tqdm(self.dataloaders['train'], total=len(self.dataloaders['train']))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{epochs}] Train\")\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        for src, tgt in pbar:\n",
    "            src = src.to(self.device)\n",
    "            tgt = tgt.to(self.device)\n",
    "            bs = src.size(1)\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=self.dtype):\n",
    "                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:-1, :], self.device)\n",
    "\n",
    "                logits = self.model(src, tgt[:-1, :], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[1:, :].reshape(-1))\n",
    "\n",
    "            running_loss += loss.item() * bs\n",
    "            total_samples += bs\n",
    "            avg_loss = running_loss / total_samples\n",
    "            pbar.set_postfix(loss=avg_loss)\n",
    "\n",
    "            # Backward\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if clip_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad_norm)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def evaluate(self, phase):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the validation or test set.\n",
    "\n",
    "        Args:\n",
    "            phase (str): Phase of evaluation ('valid' or 'test').\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple containing average token accuracy and average loss.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        pbar = tqdm(self.dataloaders[phase], total=len(self.dataloaders[phase]))\n",
    "        pbar.set_description(f\"[{self.current_epoch+1}/{epochs}] {phase.capitalize()}\")\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        running_acc_tok = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in pbar:\n",
    "                src = src.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                bs = src.size(1)\n",
    "\n",
    "                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt[:-1, :], self.device)\n",
    "                logits = self.model(src, tgt[:-1, :], src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "                loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt[1:, :].reshape(-1))\n",
    "\n",
    "                running_loss += loss.item() * bs\n",
    "                total_samples += bs\n",
    "                avg_loss = running_loss / total_samples\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def _save_model(self, checkpoint_name):\n",
    "        \"\"\"\n",
    "        Save the model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_name (str): Name of the checkpoint file.\n",
    "        \"\"\"\n",
    "        state_dict = self.model.state_dict()\n",
    "        torch.save({\n",
    "            \"epoch\": self.current_epoch + 1,\n",
    "            \"state_dict\": state_dict,\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            \"train_loss_list\": self.train_loss_list,\n",
    "            \"valid_loss_list\": self.valid_loss_list,\n",
    "        }, os.path.join(self.root_dir, checkpoint_name))\n",
    "\n",
    "    def _test_seq_acc(self,load_best=True,epochs=None):\n",
    "        \"\"\"\n",
    "        Test sequence accuracy and save results to a file.\n",
    "        \"\"\"\n",
    "        self.device = 'cuda'\n",
    "        self.load_model()\n",
    "        test_accuracy_seq = sequence_accuracy(load_best,epochs)\n",
    "        print(f\"Test Accuracy: {round(test_accuracy_seq, 4)}\")\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        if self.resume_best:\n",
    "            self.load_model(resume=True)\n",
    "        for self.current_epoch in range(self.current_epoch, epochs):\n",
    "            training_loss = self._train_epoch()\n",
    "            valid_loss = self.evaluate(\"valid\")\n",
    "\n",
    "            self.train_loss_list.append(round(training_loss, 4))\n",
    "            self.valid_loss_list.append(round(valid_loss, 4))\n",
    "            if self.save_freq:\n",
    "                if self.current_epoch % self.save_freq == 0:\n",
    "                    self._save_model(f\"model_ep{self.current_epoch + 1 }.pth\")\n",
    "            \n",
    "            if valid_loss <= self.best_val_loss:\n",
    "                self.best_val_loss = valid_loss\n",
    "                self._save_model(f\"model_best_tr.pth\")\n",
    "                self._test_seq_acc()\n",
    "\n",
    "            print(f\"Epoch {self.current_epoch + 1}/{epochs}, \"\n",
    "                  f\"Training Loss: {training_loss:.4f}, \"\n",
    "                  f\"Validation Loss: {valid_loss:.4f}, \")\n",
    "\n",
    "        if self.save_last:\n",
    "            self._save_model(f\"model_ep{self.current_epoch + 1 }.pth\")\n",
    "        self._test_seq_acc(load_best=False,epochs=self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5b346-8eab-4d88-848f-14a48dc49cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97aa0c-fb60-40ce-a6ad-65f7506c484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff373d14-77b1-4a3f-88be-efa2f7ca4bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ritesh-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

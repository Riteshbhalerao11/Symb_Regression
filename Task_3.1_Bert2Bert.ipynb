{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5038874-f6a1-4ba5-9921-039386c162ec",
   "metadata": {
    "id": "d5038874-f6a1-4ba5-9921-039386c162ec"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig, BertTokenizerFast\n",
    "from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca33c25-2dc0-46a3-8c14-450ffd959e92",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c29961-c128-4489-a682-f0f4cbeade42",
   "metadata": {
    "id": "86c29961-c128-4489-a682-f0f4cbeade42"
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "SEED = 42\n",
    "\n",
    "eq_df = pd.read_csv(\"Data/FeynmanEquations.csv\")[['Filename', 'Formula']]\n",
    "\n",
    "data_directory = 'Data/Feynman_with_units'\n",
    "# Define the maximum number of lines to read from each file\n",
    "N = 4000\n",
    "\n",
    "# Create an empty list to store tuples of (filename, line)\n",
    "data = []\n",
    "\n",
    "# Iterate over files in the data directory\n",
    "for filename in os.listdir(data_directory):\n",
    "    if os.path.isfile(os.path.join(data_directory, filename)):\n",
    "        file_path = os.path.join(data_directory, filename)\n",
    "        # Open file and read lines\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            # Append tuples of (filename, line) for each line in the file (up to N lines)\n",
    "            for line in lines[:N]:\n",
    "                data.append((filename, line))\n",
    "\n",
    "# Convert the list of tuples to a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Filename', 'features'])\n",
    "\n",
    "del data\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d07b8e-11a1-4a7a-873c-1c7bdfd21171",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6187af7-8668-48c0-839f-d2eb43e5984f",
   "metadata": {},
   "source": [
    "## Preparing Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TY_ApC7wzEF_",
   "metadata": {
    "id": "TY_ApC7wzEF_"
   },
   "outputs": [],
   "source": [
    "# Combine formulas from 'eq_df' with a range of numbers and additional characters\n",
    "corpus = eq_df.Formula.tolist() + [str(i) for i in range(10)] + [\"-\", \".\"]\n",
    "\n",
    "# Initialize BERT tokenizer from pre-trained 'bert-base-uncased' model\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Train tokenizer on the given corpus with a max vocab size of 1000\n",
    "tokenizer = tokenizer.train_new_from_iterator(corpus, vocab_size=1000)\n",
    "\n",
    "# Set beginning-of-sequence token and end-of-sequence token\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "df = pd.merge(eq_df, df, on=\"Filename\", how='inner').drop(columns=['Filename'])\n",
    "\n",
    "del eq_df, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33598f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-tokenization for ensuring character level tokens \n",
    "\n",
    "def pre_tokenize(data):\n",
    "    return data.replace(\" \", ';').replace(\"\", \" \").replace(\" ; \", tokenizer.sep_token)\n",
    "\n",
    "df['features'] = df['features'].apply(pre_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e13a6b-6672-4998-9d30-81f619f51b7e",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88610a35-4ee4-4609-9d2c-ed11fbb2a668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "    \"\"\"\n",
    "    Preprocesses data for input to the model.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data containing \"features\" and \"Formula\" keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: Processed batch with input_ids, attention_mask, decoder_input_ids,\n",
    "        decoder_attention_mask, and labels for the model.\n",
    "    \"\"\"\n",
    "    # Tokenize the inputs and labels\n",
    "    inputs = tokenizer(\n",
    "        batch[\"features\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"Formula\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    \n",
    "    # Assign tokenized inputs and labels to batch\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    \n",
    "    # Mask padding tokens in the labels\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9705149-9dd9-49d0-801d-828a7e52e675",
   "metadata": {
    "id": "e9705149-9dd9-49d0-801d-828a7e52e675"
   },
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for train, test, and validation datasets\n",
    "df_train = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "df_valid = pd.DataFrame()\n",
    "\n",
    "# Split the data into train, test, and validation sets\n",
    "for i in range(100):\n",
    "    # Sample data for the current iteration and shuffle it\n",
    "    dat = df.iloc[i * N: N * (i + 1)].sample(frac=1, random_state=SEED)\n",
    "    \n",
    "    # Calculate lengths for train, test, and validation splits\n",
    "    total_len = len(dat)\n",
    "    train_len = int(0.9 * total_len)\n",
    "    test_len = int(0.05 * total_len)  # Remaining 5% for test and valid splits\n",
    "    valid_len = total_len - train_len - test_len\n",
    "    \n",
    "    # Concatenate the splits to their respective DataFrames\n",
    "    df_train = pd.concat([df_train, dat.iloc[:train_len]])\n",
    "    df_test = pd.concat([df_test, dat.iloc[train_len:train_len + test_len]])\n",
    "    df_valid = pd.concat([df_valid, dat.iloc[train_len + test_len:]])\n",
    "\n",
    "del dat\n",
    "\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_test.reset_index(inplace=True, drop=True)\n",
    "df_valid.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Create datasets from the DataFrames\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "valid_dataset = Dataset.from_pandas(df_valid)\n",
    "\n",
    "del df_train, df_test, df_valid, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36013e4-8f64-4fd0-a017-e78ebfd891f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "f36013e4-8f64-4fd0-a017-e78ebfd891f9",
    "outputId": "d4ceb9bd-2d22-4c4f-a702-968130330332"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=512,\n",
    "    remove_columns=[\"Formula\", 'features'],\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=512,\n",
    "    remove_columns=[\"Formula\", 'features'],\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=512,\n",
    "    remove_columns=[\"Formula\", 'features'],\n",
    ")\n",
    "\n",
    "train_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")\n",
    "valid_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\",  \"labels\"],\n",
    ")\n",
    "test_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\",  \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148abdf5-3033-412c-aff8-073f9f68dd58",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd6b49-aa06-4fc7-95ed-d3428ee9c0b7",
   "metadata": {},
   "source": [
    "## Initializing model with desired configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65904087-90d0-4c50-b685-52b59bbf42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configurations for encoder and decoder\n",
    "config_encoder = BertConfig()\n",
    "config_decoder = BertConfig()\n",
    "\n",
    "# To obtain attention outputs for the trained model\n",
    "config_encoder.output_attentions = True\n",
    "config_decoder.output_attentions = True\n",
    "\n",
    "config_decoder.is_decoder = True\n",
    "config_decoder.add_cross_attention = True\n",
    "config_decoder.vocab_size = vocab_size\n",
    "\n",
    "# Modify the number of hidden layers\n",
    "config_encoder.num_hidden_layers = 3\n",
    "config_decoder.num_hidden_layers = 3\n",
    "\n",
    "# Create an encoder-decoder configuration\n",
    "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# Initialize the Encoder-Decoder model\n",
    "bert2bert = EncoderDecoderModel(config=config)\n",
    "\n",
    "# Set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Set parameters for beam search for greedy search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 30\n",
    "bert2bert.config.min_length = 0\n",
    "bert2bert.config.no_repeat_ngram_size = 0\n",
    "bert2bert.config.early_stopping = False\n",
    "bert2bert.config.length_penalty = 1.0\n",
    "bert2bert.config.num_beams = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937d95f-77eb-49b4-9812-5953570173b6",
   "metadata": {},
   "source": [
    "## Function for calculating sequence accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731631b9-4ea3-4a31-9e78-3021b35601fe",
   "metadata": {
    "id": "731631b9-4ea3-4a31-9e78-3021b35601fe"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes sequence accuracy metric for model predictions.\n",
    "\n",
    "    Args:\n",
    "        pred (EvalPrediction): The prediction object containing label_ids and predictions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed sequence accuracy.\n",
    "    \"\"\"\n",
    "    # Extract label ids and predicted ids\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Decode predicted and label sequences\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace padding tokens in label ids with pad_token_id\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute sequence accuracy\n",
    "    count = 0\n",
    "    total = len(pred_str)\n",
    "    for i in range(total):\n",
    "        if pred_str[i] == label_str[i]:\n",
    "            count += 1\n",
    "    acc = count / total\n",
    "\n",
    "    return {\"sequence_accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53395835-7d64-4aa7-a16e-6adeafad9118",
   "metadata": {},
   "source": [
    "## Preparing trainer & execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45967455-cf54-4b8b-8514-f74413b82731",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "45967455-cf54-4b8b-8514-f74413b82731",
    "outputId": "fb8c41f0-7983-4556-c1d5-b64d77286dad"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of steps per epoch\n",
    "steps = math.ceil(len(train_dataset) / 16)\n",
    "\n",
    "# Define training arguments\n",
    "trainer_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bert_min_lr_e5\",\n",
    "    fp16=True,  # Change to False if using CPU only\n",
    "    predict_with_generate=True,\n",
    "    learning_rate=5e-05,\n",
    "    num_train_epochs=30,  # The total number of training epochs to run\n",
    "    per_device_train_batch_size=16,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=32,  # Batch size for evaluation\n",
    "    report_to=\"none\",\n",
    "    evaluation_strategy=\"steps\",  # Evaluated at the end of each epoch\n",
    "    eval_steps=steps,\n",
    "    do_eval=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=steps,\n",
    "    save_total_limit=2,  # Save the best and most recent checkpoints\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=steps,\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"sequence_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    save_safetensors=False # safe_tensors had some bugs with this model\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    tokenizer=tokenizer,\n",
    "    args=trainer_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82abac7-1391-45ec-838f-2a18a7ff3557",
   "metadata": {
    "id": "c82abac7-1391-45ec-838f-2a18a7ff3557",
    "outputId": "e72596de-e5c8-46a8-98c9-091b1f9aee44"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Ritesh-kernel",
   "language": "python",
   "name": "ritesh-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
